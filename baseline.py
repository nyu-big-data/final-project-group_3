# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m0io7FsHejaLmsEiyFLzmJHKIy7AjzdA
"""

import getpass
import numpy as np
# And pyspark.sql to get the spark session
from pyspark.sql import SparkSession
from pyspark.sql import functions as func
from pyspark.sql import DataFrameStatFunctions as statFunc
from pyspark.sql.functions import isnan, when, count, col
from pyspark.mllib.evaluation import RankingMetrics

from pyspark.sql.functions import udf, struct
from pyspark.sql.types import FloatType



def main(spark, netID):
    '''Main routine for Final Project 
    Parameters
    ----------
    spark : SparkSession object
    netID : string, netID of student to find files in HDFS
    ''' 

    # Load the boats.txt and sailors.json data into DataFrame 
    ratings = spark.read.csv(f'hdfs:/user/{netID}/ratings.csv', schema='userId INT,  movieId INT, rating FLOAT, timestamp INT')
    movies = spark.read.csv(f'hdfs:/user/{netID}/movies.csv', schema='movieId INT,title STRING, genres STRING')
   
    # Give the dataframe a temporary view so we can run SQL queries
    ratings.createOrReplaceTempView('ratings')
    movies.createOrReplaceTempView('movies')

    # data quality check
    # check duplicate
    print('check duplicate')
    duplicate= ratings.groupBy("userId", "movieId").count()
    duplicate.filter(duplicate['count']>=2).show() 

    # check missing
    print('check missing')
    ratings.select([count(when(isnan(c), c)).alias(c) for c in ratings.columns]).show()
    ratings= ratings.na.drop() 

    # check data range
    print('filter out the rating within 0.5-5')
    ratings= ratings.filter(ratings.rating <=5).filter( ratings.rating >=0.5).select('userId','movieId','rating')

    print('Splitting into training, validation, and testing set based on user_Id')
    train_id, val_id, test_id = [i.rdd.flatMap(lambda x: x).collect() for i in ratings.select('userId').distinct().randomSplit([0.6, 0.2, 0.2], 1024)]
    train_dt = ratings.where(ratings.userId.isin(train_id))
    val_dt = ratings.where(ratings.userId.isin(val_id))
    test_dt = ratings.where(ratings.userId.isin(test_id)) 


    # filter out the movies with more than 10 records 
    temp= ratings.groupby('movieId').count()
    temp= temp.filter( temp['count'] >=10) 
    base_ratings= ratings.where(ratings.movieId.isin([i for i in temp.select('movieId').distinct()])) 
    
    print('Splitting into training, validation, and testing set based on user_Id')
    train_id, val_id, test_id = [i.rdd.flatMap(lambda x: x).collect() for i in base_ratings.select('userId').distinct().randomSplit([0.6, 0.2, 0.2], 1024)]
    train_dt = base_ratings.where(base_ratings.userId.isin(train_id))
    val_dt = base_ratings.where(base_ratings.userId.isin(val_id))
    test_dt = base_ratings.where(base_ratings.userId.isin(test_id)) 

    train_dt.createOrReplaceTempView('train_dt')
    val_dt.createOrReplaceTempView('val_dt')
    test_dt.createOrReplaceGlobalTempView('test_dt')

    rating_num= train_dt.groupBy('movieId', 'rating').count()  
    mul_udf = udf(lambda x, y: x*y , FloatType()) 
    rating_num = rating_num.withColumn("result", mul_udf(rating_num['rating'], rating_num['count']))


    rating_num.createOrReplaceTempView('rating_num')
    top_1000_movie= spark.sql('select movieId, cast(sum(result) as float)/sum(count)  weight_socre \
                                from  rating_num \
                                group by movieId \
                                order by weight_socre desc limit 1000')

    filter_val = val_dt.where(val_dt.movieId.isin([i for i in train_dt.select('movieId').distinct()])) 
    filter_test = test_dt.where(test_dt.movieId.isin([i for i in train_dt.select('movieId').distinct()]))
    
    # result on the val data
    val_users = filter_val.select("userId").distinct()
    rec_list = top_1000_movie.select(top_1000_movie.movieId).agg(func.collect_list('movieId')) 


    rec= val_users.rdd.cartesian(rec_list.rdd).map(lambda row: (row[0][0], row[1][0])).toDF() 
    pred = rec.select(rec._1.alias('userId'), rec._2.alias('pred')) 
    true = filter_val.groupby('userId').agg(func.collect_set('movieId').alias('true'))

    predAndtrue = pred.join(true, 'userId').rdd.map(lambda row: (row[1], row[2]))


    val_map = RankingMetrics(predAndtrue).meanAveragePrecision 

    # results on test set
    test_users = filter_test.select("userId").distinct()
    rec_list = top_1000_movie.select(top_1000_movie.movieId).agg(func.collect_list('movieId')) 

    ### error starts here:!!!!!
    rec= test_users.rdd.cartesian(rec_list.rdd).map(lambda row: (row[0][0], row[1][0])).toDF() 
    pred = rec.select(rec._1.alias('userId'), rec._2.alias('pred')) 
    true = filter_test.groupby('userId').agg(func.collect_set('movieId').alias('true'))

    predAndtrue = pred.join(true, 'userId').rdd.map(lambda row: (row[1], row[2]))
    test_map = RankingMetrics(predAndtrue).meanAveragePrecision 

    # Use Mean Average Precision as evaluation metric
    print(f'MAP on validation set = {val_map}') 
    print(f'MAP on test set = {test_map}') 

if __name__ == "__main__":

    # Create the spark session object
    # spark = SparkSession.builder.appName('part1').getOrCreate()
    spark = SparkSession.builder.getOrCreate()

    # # Get user netID from the command line
    netID = getpass.getuser()

    # Call our main routine
    main(spark, netID)